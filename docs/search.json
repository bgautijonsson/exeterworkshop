[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This text comes before the aboutme\nThis text comes after the aboutme"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About",
    "section": "Education",
    "text": "Education\nUniversity of Iceland | Reykjavík, Iceland\nMsc in Statistics | September 2018 - September 2020\nUniversity of Iceland Reykjavík, Iceland"
  },
  {
    "objectID": "posts/forest-plot-table/index.html",
    "href": "posts/forest-plot-table/index.html",
    "title": "Forest Plots with Built-In Tables",
    "section": "",
    "text": "I’ve seen this kind of figure poking around, but I didn’t really think about them until the other day I was asked about how to make one in R. Here I will walk through making one of these plots using the ggplot2 and cowplot packages.\nTo start with I have some fake data, d.\n\n\nCode\nd |&gt; \n    kable(format = \"html\") |&gt; \n    kable_styling(full_width = F)\n\n\n\n\n\nname\nmean\nlower\nupper\n\n\n\n\nCovfefe\n1.1989430\n1.0707825\n1.358700\n\n\nCoffee\n1.8509518\n1.5589701\n2.227752\n\n\nVariable\n1.2181512\n1.0834045\n1.411683\n\n\nCovariate\n1.1970336\n1.0699671\n1.342737\n\n\nPredictor\n0.9085823\n0.8049225\n1.000000\n\n\nSmoking\n0.8770238\n0.7447709\n1.000000\n\n\nAge\n0.9344551\n0.8305984\n1.000000\n\n\nUranium\n0.9177338\n0.8021060\n1.000000\n\n\nStuff\n1.0760010\n1.0000000\n1.216474\n\n\nThing\n0.9143605\n0.7753231\n1.000000\n\n\nKoffing\n1.0686924\n1.0000000\n1.178642\n\n\nCoughing\n1.0648236\n1.0000000\n1.194998\n\n\nPolvo\n0.9227277\n0.8097846\n1.000000\n\n\n\n\n\n\n\n\nThe Forest Plot\nThe forest plot itself is not hard to do. Notice how I create this striped pattern with geom_vline(aes(xintercept = name), col = \"grey95\", size = 5). I’ll do the same thing to the table part of the figure later.\n\n\nCode\np1 &lt;- d |&gt; \n    mutate(name = fct_reorder(name, mean)) |&gt; \n    ggplot(aes(x = name, y = mean,\n                  ymin = lower, ymax = upper)) +\n    geom_vline(aes(xintercept = name), col = \"grey95\", size = 5) +\n    geom_hline(yintercept = 1, lty = 2) +\n    geom_point() +\n    geom_linerange() +\n    coord_flip() +\n    labs(x = NULL, y = NULL) +\n    theme(plot.margin = margin(t = 5, r = -4, b = 5, l = 5))\n\np1\n\n\n\n\n\n\n\n\n\n\n\nThe Table\nTo make the table look nice I convert the numbers to text and clean them up so that the text is justified nicely when plotting it. I did some manual tuning of the margins to make the plot and table line up nicely.\n\n\nCode\np2 &lt;- d |&gt; \n    mutate(name = fct_reorder(name, mean),\n           mean = round(mean, 2),\n           mean = str_pad(mean, width = 4, side = \"right\", pad = \"0\"),\n           lower = round(lower, 2),\n           lower = as.character(lower),\n           lower = ifelse(lower == \"1\", \"1.00\", lower),\n           lower = str_pad(lower, width = 4, side = \"right\", pad = \"0\"),\n           upper = round(upper, 2),\n           upper = as.character(upper),\n           upper = ifelse(upper == \"1\", \"1.00\", upper),\n           upper = str_pad(upper, width = 4, side = \"right\", pad = \"0\"),\n           ci = str_c(lower, \", \", upper)) |&gt; \n    ggplot(aes(x = name)) +\n    geom_vline(aes(xintercept = name), col = \"grey95\", size = 5) +\n    geom_text(aes(label = mean, y = 1)) +\n    geom_text(aes(label = ci, y = 1.07)) +\n    coord_flip(ylim = c(0.99, 1.1)) +\n    theme(axis.title = element_blank(),\n          axis.text = element_blank(),\n          axis.ticks = element_blank(),\n          axis.line.y = element_blank(),\n          panel.border = element_blank(), \n          panel.grid = element_blank(), \n          plot.background = element_blank(),\n          plot.margin = margin(t = 5, r = 5, b = 19.3, l = 0))\n\np2\n\n\n\n\n\n\n\n\n\n\nThe finished bottom row\nNow we have alligned the plot and table. If you zoom in you might see that they don’t line up perfectly. I am not liable for any bodily harm caused by this.\n\n\nCode\nbottom_row &lt;- plot_grid(p1, p2, nrow = 1, rel_widths = c(1, 0.3)) \n\nbottom_row\n\n\n\n\n\n\n\n\n\n\n\n\nThe headers\nI admit that this is a pretty handwavy manual way to make the header fit, and there is probably a nice way to automatically fit this using the coordinate system.\n\n\nCode\ntop_row &lt;- ggplot(data = tibble()) +\n    geom_text(aes(y = 1, x = 0.02, label = \"Variable\"), size = 5) +\n    geom_text(aes(y = 1, x = 0.84, label = \"Mean\"), size = 5) +\n    geom_text(aes(y = 1, x = 0.98, label = \"95% CI\"), size = 5) +\n    coord_cartesian(xlim = c(0, 1)) +\n    theme_void() +\n    theme(plot.margin = margin(t = 0, r = 5, b = 0, l = 5),\n          axis.line.x.bottom = element_line())\n\n\ntop_row\n\n\n\n\n\n\n\n\n\n\n\nPutting it all together\nAnd so we come to the finished plot. If we wrangle the rel_heights setting a little bit we can get a pretty nice looking forest plot and table hybrid.\n\n\nCode\nplot_grid(top_row, bottom_row, ncol = 1, rel_heights = c(0.07, 1))\n\n\n\n\n\n\n\n\n\nThis was actually less of an inconvenience than I thought. The mixture of ggplot2 and cowplot made this a pretty easy task."
  },
  {
    "objectID": "posts/gev_copula_ar1/index.html",
    "href": "posts/gev_copula_ar1/index.html",
    "title": "Applying a Gaussian AR(1) Copula to Generalized Extreme Value Margins",
    "section": "",
    "text": "Code\nlibrary(gt)\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(bggjphd)\ntheme_set(theme_bggj())"
  },
  {
    "objectID": "posts/gev_copula_ar1/index.html#copulas",
    "href": "posts/gev_copula_ar1/index.html#copulas",
    "title": "Applying a Gaussian AR(1) Copula to Generalized Extreme Value Margins",
    "section": "Copulas",
    "text": "Copulas\nIn the book Elements of Copula Modeling with R, Hofert et al. (2018) define a copula as\n\na multivariate distribution function with standard uniform univariate margins, that is, U(0, 1) margins.\n\nSo, a Copula is any multivariate distribution whose margins is U(0,1 ) distributed. A simple example is the independence copula\n\\[\n\\Pi(u) = \\prod_{j=1}^{d}{u_j}, \\quad \\mathbf U \\in [0, 1]^d.\n\\]\nThe central theorem of copula theory is Sklar’s theorem (Sklar 1996) [the original paper is from 1959]. The theorem basically states that for any d-dimensional distribution function, \\(H\\), with univariate margins \\(F_1, \\dots, F_d\\), there exists a d-dimensional copula \\(C\\) such that\n\\[\nH(\\mathbf x) = C(F_1(x_1), \\dots, F_d(x_d)).\n\\]\nAlternatively, we can write\n\\[\nC(\\mathbf u) = H(F_1^{-1}(u_1), \\dots, F_d^{-1}(u_d))\n\\]\n\nGaussian Copula\nThe Gaussian family of copulas can be written\n\\[\nC(\\mathbf u | R) = \\Phi_d\\left(\\Phi^{-1}(u_1), \\dots, \\Phi^{-1}(u_d) \\vert R \\right), \\quad \\mathbf u \\in [0, 1]^d,\n\\]\nwhere R is a \\(d\\times d\\) correlation matrix, \\(\\Phi_d\\) is the CDF of a d-dimensional multivariate Gaussian with mean \\(\\mathbf 0\\) and covariance matrix \\(R\\), and \\(\\Phi\\) is the CDF of a standard Gaussian.\nIts density is\n\\[\nc(\\mathbf u \\vert R) = \\frac{\\phi_d\\left(\\Phi^{-1}(u_1), \\dots, \\Phi^{-1}(u_d) \\vert R \\right)}{ \\prod_{j=1}^{d}{\\phi\\left( \\Phi^{-1}(u_j) \\right)}}, \\quad \\mathbf u \\in [0, 1]^d,\n\\]\nwhere \\(\\phi_d\\) is the density of the same multivariate Gaussian, and \\(\\phi\\) is the density of a standard gaussian.\n\nSampling from the Gaussian copula\nGiven the \\(d\\times d\\) correlation matrix, \\(R\\):\n\nCompute the Cholesky factor \\(L\\) of the correlation matrix, \\(R\\).\nSample \\(Z_1, \\dots, Z_d \\overset{iid}{\\sim}\\mathrm{Normal}(0, 1)\\).\nCompute \\(X = LZ\\).\nReturn \\(\\mathbf u = \\left(\\Phi(X_1), \\dots, \\Phi(X_d)\\right)\\)\n\nWe can then make the margins \\(u_1, \\dots, u_d\\) follow any distribution (f.ex. the GEV distribution) by applying its quantile function.\n\n\n\nExample\nLet’s sample from a two-dimensional process with GEV(8, 2, 0.05) margins and a Gaussian copula with correlation \\(\\rho = 0.7\\).\n\n\nCode\nR &lt;- matrix(c(1, 0.7, 0.7, 1), nrow = 2)\nset.seed(1)\nX &lt;- mvtnorm::rmvnorm(\n  n = 200,\n  sigma = R\n)\n\nX |&gt; \n  as_tibble() |&gt; \n  mutate(\n    id = row_number()\n  ) |&gt; \n  pivot_longer(c(-id), names_to = \"variable\", values_to = \"Z\") |&gt; \n  mutate(\n    U = pnorm(Z),\n    Y = evd::qgev(U, loc = 8, scale = 2, shape = 0.05)\n  ) |&gt; \n  pivot_longer(c(-id, -variable)) |&gt; \n  pivot_wider(names_from = variable, values_from = value) |&gt;\n  mutate(\n    name = fct_relevel(name, \"Z\", \"U\", \"Y\") |&gt; \n      fct_recode(\n        \"Z (Gaussian)\" = \"Z\",\n        \"U (Uniform)\" = \"U\",\n        \"Y (GEV)\" = \"Y\"\n      )\n  ) |&gt; \n  group_by(name2 = name) |&gt; \n  group_map(\n    function(data, ...) {\n      data |&gt; \n        ggplot(aes(V1, V2)) +\n        geom_density_2d_filled() +\n        geom_point(col = \"grey70\") +\n        coord_cartesian(expand = FALSE) +\n        theme(legend.position = \"none\") +\n        labs(\n          x = NULL,\n          y = NULL,\n          subtitle = data$name\n        )\n    }\n  ) |&gt; \n  wrap_plots() +\n  plot_annotation(\n    title = \"The stages in generating 2d GEV(8, 2, 0.05) data with a Gaussian correlation 0.7\"\n  )"
  },
  {
    "objectID": "posts/gev_copula_ar1/index.html#data",
    "href": "posts/gev_copula_ar1/index.html#data",
    "title": "Applying a Gaussian AR(1) Copula to Generalized Extreme Value Margins",
    "section": "Data",
    "text": "Data\nR scripts used for simulating the data\nThe simulated data have GEV margins and an AR(1) Gaussian copula. The steps in simulating the data are as follows.\n\nGenerate a correlation matrix, R, based on an AR(1) process\nGenerate multivariate normal variates with mean zero and covariance matrix R.\nTransform to GEV by applying normal CDF and GEV quantile functions\n\n\n1. Correlation Matrix\nFirst, a correlation matrix is generated that encodes an AR(1) process with correlation \\(\\rho\\). To create this correlation matrix, we specify the precision matrix, \\(Q\\), via\n\\[\nQ = \\frac{1}{1 - \\rho^2}\\begin{bmatrix}\n1 & -\\rho & \\dots & \\dots & \\vdots\\\\\n-\\rho & 1 + \\rho^2 & -\\rho & \\ddots & \\vdots \\\\\n\\vdots & \\ddots & \\ddots & \\ddots & \\vdots \\\\\n\\vdots & \\ddots &-\\rho & 1 + \\rho^2 & -\\rho \\\\\n\\vdots & \\dots & \\dots & -\\rho & 1\n\\end{bmatrix}.\n\\]\nThis gives us the correlation matrix, R, where\n\\[\nR = Q^{-1} = \\begin{bmatrix}\n1 & \\rho & \\rho^2 & \\dots & \\rho^n\\\\\n\\rho & 1 & \\rho & \\ddots & \\rho^{n-1} \\\\\n\\vdots & \\ddots & \\ddots & \\ddots & \\vdots \\\\\n\\rho^{n-1} & \\ddots &\\rho & 1 & \\rho \\\\\n\\rho^n & \\dots & \\dots & \\rho & 1\n\\end{bmatrix}.\n\\]\n\n\nCode\nmake_AR_cor_matrix_1d &lt;- function(n_id, rho = 0.5) {\n  P &lt;- matrix(\n    0, \n    nrow = n_id,\n    ncol = n_id\n  )\n  diag(P) &lt;- 1\n  for (i in seq(1, n_id - 1)) {\n    P[i, i + 1] &lt;- -rho\n  }\n  \n  for (i in seq(2, n_id)) {\n    P[i, i - 1] &lt;- -rho\n  }\n  \n  for (i in seq(2, n_id - 1)) {\n    P[i, i] &lt;- 1 + rho^2\n  }\n  \n  P &lt;- P / (1 - rho^2)\n  \n  P_cor &lt;- solve(P)\n  P_cor\n}\n\nn_id &lt;- 5\nrho &lt;- 0.5\n\nR &lt;- make_AR_cor_matrix_1d(n_id = n_id, rho = rho)\nR\n\n\n       [,1]  [,2] [,3]  [,4]   [,5]\n[1,] 1.0000 0.500 0.25 0.125 0.0625\n[2,] 0.5000 1.000 0.50 0.250 0.1250\n[3,] 0.2500 0.500 1.00 0.500 0.2500\n[4,] 0.1250 0.250 0.50 1.000 0.5000\n[5,] 0.0625 0.125 0.25 0.500 1.0000\n\n\n\n\n2. Generating Multivariate Normal Data\nWe then use the well-known fact that if\n\\[\n\\mathbf X \\sim \\mathrm{MVNorm}\\left(\\boldsymbol \\mu, \\Sigma\\right),\n\\]\nthen\n\\[\n\\boldsymbol X = \\boldsymbol \\mu +  L \\boldsymbol Z,\n\\]\nwhere \\(\\boldsymbol Z \\sim \\mathrm{Normal}(\\boldsymbol 0,  I)\\) and \\(L\\) is the Cholesky decomposition of \\(R\\), or \\(LL^T = \\Sigma\\).\nIn our case, we want \\(\\mathbf X\\) to have mean 0 and covariance matrix \\(R\\), so we write\n\\[\n\\mathbf X = L\\mathbf Z\n\\]\nwhere \\(LL^T = R\\).\nWe can skip the manual Cholesky factorization and simply use the {mvtnorm} package.\n\n\nCode\nsample_gaussian_variables &lt;- function(cor_matrix, n_replicates) {\n  mvtnorm::rmvnorm(\n    n = n_replicates,\n    sigma = cor_matrix\n  )\n}\n\nR |&gt; \n  sample_gaussian_variables(n_replicates = 1)\n\n\n         [,1]     [,2]       [,3]       [,4]       [,5]\n[1,] 1.427875 1.837133 -0.1348893 -0.3970323 -0.4613493\n\n\nWe will want more than one observation from each “site”, so we write a helped function for tidying this output into a tibble.\n\n\nCode\ntidy_mvgauss &lt;- function(mvnorm_matrix) {\n  colnames(mvnorm_matrix) &lt;- seq_len(ncol(mvnorm_matrix))\n  \n  mvnorm_matrix |&gt; \n    dplyr::as_tibble() |&gt; \n    dplyr::mutate(\n      replicate = dplyr::row_number(),\n      .before = `1`\n    ) |&gt; \n    tidyr::pivot_longer(\n      c(-replicate),\n      names_to = \"id\", names_transform = as.numeric,\n      values_to = \"Z\"\n    )\n  \n}\n\nn_replicates &lt;- 10\nmvnorm_data &lt;- R |&gt; \n  sample_gaussian_variables(n_replicates = n_replicates) |&gt; \n  tidy_mvgauss()\n\nmvnorm_data |&gt; \n  filter(replicate &lt;= 5) |&gt; \n  pivot_wider(names_from = replicate, values_from = Z) |&gt; \n  gt() |&gt; \n  tab_spanner(\n    columns = -id,\n    label = \"Replicate\"\n  )\n\n\n\n\n\n\n\n\nid\nReplicate\n\n\n1\n2\n3\n4\n5\n\n\n\n\n1\n-0.4364007\n-0.1443106\n0.7080631\n1.1388723\n1.0330390\n\n\n2\n-0.4017539\n0.6923541\n0.9544816\n-0.4595604\n-1.3986486\n\n\n3\n-0.1161437\n0.9328314\n-0.1228664\n-0.4689164\n-1.1884398\n\n\n4\n1.0500406\n-0.5297770\n-0.8530328\n-1.5227378\n-1.0434809\n\n\n5\n-0.3607882\n-0.5568922\n1.3144552\n0.1006016\n-0.9255078\n\n\n\n\n\n\n\n\n\n3. Transforming to Multivariate GEV\nNow our data, \\(\\mathbf X\\), is multivariate normal with dependence structure according to an AR(1) process, but marginally each \\(X_1\\) is standard normal. To transform this data to a GEV dataset, we simply use the standard normal CDF on each \\(X_i\\) to transform it to \\([0, 1]\\), then we use the GEV quantile function to transform that to a GEV distributed variable.\n\n\nCode\nmvnorm_to_gev &lt;- function(mvnorm_data, gev_params) {\n  \n  mvnorm_data |&gt; \n    dplyr::mutate(\n      U = pnorm(Z)\n    ) |&gt; \n    dplyr::inner_join(\n      gev_params,\n      by = dplyr::join_by(id)\n    ) |&gt; \n    dplyr::mutate(\n      y = purrr:::pmap_dbl(\n        list(U, mu, sigma, xi), \n        \\(U, mu, sigma, xi) evd::qgev(p = U, loc = mu, scale = sigma, shape = xi)\n      )\n    )\n}\n\ngev_params &lt;- expand_grid(\n  mu = 6,\n  sigma = 3,\n  xi = 0.1,\n  id = seq_len(n_id)\n)\n\ngev_data &lt;- mvnorm_data |&gt; \n  mvnorm_to_gev(gev_params = gev_params)\n\ngev_data |&gt; \n  select(-mu, -sigma, -xi) |&gt; \n  gt() |&gt; \n  opt_interactive()"
  },
  {
    "objectID": "posts/gev_copula_ar1/index.html#checking-the-data",
    "href": "posts/gev_copula_ar1/index.html#checking-the-data",
    "title": "Applying a Gaussian AR(1) Copula to Generalized Extreme Value Margins",
    "section": "Checking the data",
    "text": "Checking the data\nLet’s create a function that plots heat maps for each of the three representations of the data\n\nZ: The multivariate normal\nU: The multivariate uniform\ny: The multivariate GEV\n\nWe want the color scales to be free, so we use group_map() and wrap_plots() instead of facet_wrap()\n\n\nCode\nplot_data &lt;- function(data) {\n  data |&gt; \n    select(replicate, id, Z, U, y) |&gt; \n    pivot_longer(c(-replicate, -id)) |&gt; \n    mutate(\n      name = fct_relevel(name, \"Z\", \"U\", \"y\"),\n      name2 = name\n    ) |&gt; \n    group_by(name) |&gt; \n    group_map(\n      function(data, ...) {\n        data |&gt; \n          ggplot(aes(id, replicate, fill = value)) +\n          geom_raster() +\n          scale_fill_viridis_c() +\n          coord_cartesian(expand = FALSE) +\n          theme(legend.position = \"none\") +\n          labs(\n            subtitle = data$name2\n          )\n      }\n    ) |&gt; \n    wrap_plots() +\n    plot_layout(nrow = 1) +\n    plot_annotation(\n      title = \"Heatmaps of the three data representations\",\n      subtitle = \"Z: Normal | U: Uniform | y: GEV\"\n    )\n}\n\n\n\n\nCode\ngev_data |&gt; \n  plot_data()\n\n\n\n\n\n\n\n\n\nIt’s nice to wrap this process in a single function\n\n\nCode\nmake_data &lt;- function(gev_params, n_replicate, rho) {\n  \n  make_AR_cor_matrix_1d(n_id = nrow(gev_params), rho = rho) |&gt; \n    sample_gaussian_variables(n_replicates = n_replicate) |&gt; \n    tidy_mvgauss() |&gt; \n    dplyr::mutate(\n      U = pnorm(Z)\n    ) |&gt; \n    dplyr::inner_join(\n      gev_params,\n      by = dplyr::join_by(id)\n    ) |&gt; \n    dplyr::mutate(\n      y = purrr::pmap_dbl(\n        list(U, mu, sigma, xi), \n        \\(U, mu, sigma, xi) evd::qgev(p = U, loc = mu, scale = sigma, shape = xi)\n      )\n    )\n}\n\n\nLet’s see how the data look if we make it larger and increase the correlation\n\n\nCode\nn_id &lt;- 40\nn_replicates &lt;- 100\n\ngev_params &lt;- expand_grid(\n  mu = 6,\n  sigma = 3,\n  xi = 0.1,\n  id = seq_len(n_id)\n)\n\nd &lt;- make_data(gev_params, n_replicates, rho = 0.95)\n\n\nWe see that with higher neighbor correlations we get obvious horizontal stripes corresponding to a large amount of dependence within each replicate.\n\n\nCode\nd |&gt; plot_data()"
  },
  {
    "objectID": "posts/gev_copula_ar1/index.html#modeling",
    "href": "posts/gev_copula_ar1/index.html#modeling",
    "title": "Applying a Gaussian AR(1) Copula to Generalized Extreme Value Margins",
    "section": "Modeling",
    "text": "Modeling\nR scripts used for modeling\n\nAR(1) Model\nWe can fit the AR(1) model directly using Stan.\n\n\nCode\nfunctions {\n  real normal_ar1_lpdf(vector x, real rho) {\n    int N = num_elements(x);\n    real out;\n    real log_det = - (N - 1) * (log(1 + rho) + log(1 - rho)) / 2;\n    vector[N] q;\n    real scl = sqrt(1 / (1 - rho^2));\n    \n    q[1:(N - 1)] = scl * (x[1:(N - 1)] - rho * x[2:N]);\n    q[N] = x[N];\n    \n    out = log_det - dot_self(q) / 2;\n    \n    return out;\n  }\n\n  real normal_copula_ar1_lpdf(vector U, real rho) {\n    int N = rows(U);\n    vector[N] Z = inv_Phi(U);\n    return normal_ar1_lpdf(Z | rho) + dot_self(Z) / 2;\n  }\n\n  real gev_cdf(real y, real mu, real sigma, real xi) {\n    if (abs(xi) &lt; 1e-10) {\n      real z = (y - mu) / sigma;\n      return exp(-exp(z));\n    } else {\n      real z = 1 + xi * (y - mu) / sigma;\n      if (z &gt; 0) {\n        return exp(-pow(z, -1/xi));\n      } else {\n        reject(\"Found incompatible GEV parameter values\");\n      }\n    }\n  } \n\n  real gev_lpdf(real y, real mu, real sigma, real xi) {\n    if (abs(xi) &lt; 1e-10) {\n      real z = (y - mu) / sigma;\n      return -log(sigma) - z - exp(-z);\n    } else {\n      real z = 1 + xi * (y - mu) / sigma;\n      if (z &gt; 0) {\n        return -log(sigma) - (1 + 1/xi) * log(z) - pow(z, -1/xi);\n      } else {\n        reject(\"Found incompatible GEV parameter values\");\n      }\n    }\n  }\n}\n\ndata {\n  int&lt;lower = 0&gt; n_replicate;\n  int&lt;lower = 0&gt; n_id;\n  array[n_replicate, n_id] real y;\n  array[n_replicate, n_id] real y_test;\n}\n\nparameters {\n  real&lt;lower = 0&gt; mu;\n  real&lt;lower = 0&gt; sigma;\n  real&lt;lower = -0.5, upper = 1&gt; xi;\n  real&lt;lower = -1, upper = 1&gt; rho;\n}\n\nmodel {\n  for (i in 1:n_replicate) {\n    vector[n_id] U;\n    for (j in 1:n_id) {    \n      U[j] = gev_cdf(y[i, j] | mu, sigma, xi);\n      target += gev_lpdf(y[i, j] | mu, sigma, xi);\n    } \n    target += normal_copula_ar1_lpdf(U | rho);\n  }\n}\n \ngenerated quantities {\n  real log_lik = 0;\n  {\n    \n    for (i in 1:n_replicate) {\n      vector[n_id] U;\n      for (j in 1:n_id) {\n        U[j] = gev_cdf(y_test[i, j] | mu, sigma, xi);\n        log_lik += gev_lpdf(y_test[i, j] | mu, sigma, xi);\n      }\n      log_lik += normal_copula_ar1_lpdf(U | rho);\n    }\n  }\n  \n}\n\n\n\n\ni.i.d. Model\n\n\nCode\nfunctions {\n  real gev_lpdf(real y, real mu, real sigma, real xi) {\n    if (abs(xi) &lt; 1e-10) {\n      real z = (y - mu) / sigma;\n      return -log(sigma) - z - exp(-z);\n    } else {\n      real z = 1 + xi * (y - mu) / sigma;\n      if (z &gt; 0) {\n        return -log(sigma) - (1 + 1/xi) * log(z) - pow(z, -1/xi);\n      } else {\n        reject(\"Found incompatible GEV parameter values\");\n      }\n    }\n  }\n}\n\ndata {\n  int&lt;lower = 0&gt; n_replicate;\n  int&lt;lower = 0&gt; n_id;\n  array[n_replicate, n_id] real y;\n  array[n_replicate, n_id] real y_test;\n}\n\nparameters {\n  real&lt;lower = 0&gt; mu;\n  real&lt;lower = 0&gt; sigma;\n  real&lt;lower = -0.5, upper = 1&gt; xi;\n}\n\ntransformed parameters {\n  \n}\n\nmodel {\n  for (i in 1:n_replicate) {\n    for (j in 1:n_id) {\n      target += gev_lpdf(y[i, j] | mu, sigma, xi);\n    }\n  }\n}\n\ngenerated quantities {\n  real log_lik = 0;\n  {\n    \n    for (i in 1:n_replicate) {\n      for (j in 1:n_id) {\n        log_lik += gev_lpdf(y_test[i, j] | mu, sigma, xi);\n      }\n    }\n  }\n}\n\n\n\n\nTwo-Step Estimation\nWe can’t fit this model directly using one stan file. What we do is:\n\nEstimate the marginal distributions\nUse the empirical CDFs to transform the data into uniformly distributed variables\nTransform these uniformly distributed variables into standard normal variables\nEstimate the precision matrix using these variables\nMake sure the precision matrix is semi-positive definite and normalize it so that its inverse is a correlation matrix\nRe-estimate the marginal distributions with a gaussian copula assuming this precision matrix is known\n\n\n\nCode\nfunctions {\n  real gev_cdf(real y, real mu, real sigma, real xi) {\n    if (abs(xi) &lt; 1e-10) {\n      real z = (y - mu) / sigma;\n      return exp(-exp(z));\n    } else {\n      real z = 1 + xi * (y - mu) / sigma;\n      if (z &gt; 0) {\n        return exp(-pow(z, -1/xi));\n      } else {\n        reject(\"Found incompatible GEV parameter values\");\n      }\n    }\n  }\n\n  real gev_lpdf(real y, real mu, real sigma, real xi) {\n    if (abs(xi) &lt; 1e-10) {\n      real z = (y - mu) / sigma;\n      return -log(sigma) - z - exp(-z);\n    } else {\n      real z = 1 + xi * (y - mu) / sigma;\n      if (z &gt; 0) {\n        return -log(sigma) - (1 + 1/xi) * log(z) - pow(z, -1/xi);\n      } else {\n        reject(\"Found incompatible GEV parameter values\");\n      }\n    }\n  }\n\n  real normal_prec_chol_lpdf(vector x, array[] int n_values, array[] int index, vector values, real log_det) {\n    int N = num_elements(x);\n    int counter = 1;\n    vector[N] q = rep_vector(0, N);\n\n    for (i in 1:N) {\n      for (j in 1:n_values[i]) {\n        q[i] += values[counter] * x[index[counter]];\n        counter += 1;\n      }\n    }\n\n    return log_det - dot_self(q) / 2;\n\n  }\n\n  real normal_copula_prec_chol_lpdf(vector U, array[] int n_values, array[] int index, vector value, real log_det) {\n    int N = rows(U);\n    vector[N] Z = inv_Phi(U);\n\n    return normal_prec_chol_lpdf(Z | n_values, index, value, log_det) + dot_self(Z) / 2;\n  }\n\n}\n\ndata {\n  int&lt;lower = 0&gt; n_replicate;\n  int&lt;lower = 0&gt; n_id;\n  array[n_replicate, n_id] real y;\n  array[n_replicate, n_id] real y_test;\n\n  int&lt;lower = 0&gt; n_nonzero_chol_Q;\n  real log_det_Q;\n  array[n_id] int n_values;\n  array[n_nonzero_chol_Q] int index;\n  vector[n_nonzero_chol_Q] value;\n}\n\nparameters {\n  real&lt;lower = 0&gt; mu;\n  real&lt;lower = 0&gt; sigma;\n  real&lt;lower = -0.5, upper = 1&gt; xi;\n}\n\nmodel {\n  for (i in 1:n_replicate) {\n    for (j in 1:n_id) {\n      target += gev_lpdf(y[i, j] | mu, sigma, xi);\n    }\n  }\n}\n \ngenerated quantities {\n  real log_lik = 0;\n  {\n    \n    for (i in 1:n_replicate) {\n      vector[n_id] U;\n      for (j in 1:n_id) {\n        U[j] = gev_cdf(y_test[i, j] | mu, sigma, xi);\n        log_lik += gev_lpdf(y_test[i, j] | mu, sigma, xi);\n      }\n      log_lik += normal_copula_prec_chol_lpdf(U | n_values, index, value, log_det_Q);\n    }\n  }\n  \n}"
  },
  {
    "objectID": "posts/gev_copula_ar1/index.html#estimates-of-gev-parameters",
    "href": "posts/gev_copula_ar1/index.html#estimates-of-gev-parameters",
    "title": "Applying a Gaussian AR(1) Copula to Generalized Extreme Value Margins",
    "section": "Estimates of GEV Parameters",
    "text": "Estimates of GEV Parameters"
  },
  {
    "objectID": "posts/gev_copula_ar1/index.html#expected-log-predicted-probability-density",
    "href": "posts/gev_copula_ar1/index.html#expected-log-predicted-probability-density",
    "title": "Applying a Gaussian AR(1) Copula to Generalized Extreme Value Margins",
    "section": "Expected log predicted probability density",
    "text": "Expected log predicted probability density\n\n\nComparing to the simple i.i.d. model"
  },
  {
    "objectID": "posts/gev_copula_ar1/index.html#differences-between-ar1-and-two-step-model",
    "href": "posts/gev_copula_ar1/index.html#differences-between-ar1-and-two-step-model",
    "title": "Applying a Gaussian AR(1) Copula to Generalized Extreme Value Margins",
    "section": "Differences between AR(1) and Two-Step Model",
    "text": "Differences between AR(1) and Two-Step Model"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Binni's Webpage!",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n\n\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nFeb 8, 2024\n\n\nBrynjólfur Gauti Guðrúnar Jónsson\n\n\n\n\n\n\n\n\n\n\n\n\nApplying a Gaussian AR(1) Copula to Generalized Extreme Value Margins\n\n\nA simulation study of the differences between AR(1) copula and i.i.d copula\n\n\n\nenglish\n\n\nR\n\n\nphd\n\n\nstan\n\n\nbayesian\n\n\nspatial statistics\n\n\ncopulas\n\n\n\nAnother step in my PhD studies is to apply multivariate copulas to data with Generalized Extreme Value marginal distributions. This is important to enable us to model dependence on the data level, as opposed to the latent (parameter) level. \n\n\n\n\n\nFeb 6, 2024\n\n\nBrynjólfur Gauti Guðrúnar Jónsson\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nFeb 5, 2024\n\n\nBrynjólfur Gauti Guðrúnar Jónsson\n\n\n\n\n\n\n\n\n\n\n\n\nForest Plots with Built-In Tables\n\n\nCreating forest plots with coefficient tables in ggplot2 with cowplot\n\n\n\nenglish\n\n\nR\n\n\nplots\n\n\ntutorial\n\n\n\nI’ve seen this kind of figure poking around, but I didn’t really think about them until the other day I was asked about how to make one in R. Here I will walk through making one of these plots using the ggplot2 and cowplot packages. \n\n\n\n\n\nJun 28, 2022\n\n\nBrynjólfur Gauti Guðrúnar Jónsson\n\n\n\n\n\n\nNo matching items"
  }
]